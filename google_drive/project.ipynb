{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSCI-511 - Final Project - Group 1\n",
    "\n",
    "#### Financial Market Data Store\n",
    "\n",
    "Data stored in Google Drive: [Access Link](https://drive.google.com/drive/folders/1hgWRHwlC9thoPKd7-dRqEHTPGYv3LtUk?usp=sharing)\n",
    "\n",
    "##### Software Pre-requisites:\n",
    "```\n",
    "pip install pandas\n",
    "pip install finnhub-python\n",
    "pip install google-api-python-client\n",
    "pip install gspread\n",
    "pip install gspread-dataframe\n",
    "pip install google-auth\n",
    "pip install google-auth-oauthlib\n",
    "```\n",
    "\n",
    "##### Running instructions \n",
    "- Press `Run All` Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import finnhub\n",
    "import gspread\n",
    "import gspread_dataframe as gd\n",
    "import pandas as pd\n",
    "import base64\n",
    "from datetime import date, timedelta\n",
    "from google.oauth2.service_account import Credentials\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "# dates for request parameters\n",
    "start_time = time.time()\n",
    "date_today = date.today()\n",
    "today_date = date_today.strftime('%Y-%m-%d')\n",
    "current_year = str(date_today.year)\n",
    "\n",
    "from_time_unix = int(time.mktime((date_today - timedelta(weeks = 52)).timetuple()))\n",
    "to_time_unix = int(time.mktime(date_today.timetuple()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finnhub clients setup - https://finnhub.io/docs/api/\n",
    "def generate_finnhub_clients():\n",
    "    finnhub_keys = ['Y2Q0b3FpYWFkM2k5OGpodTJwZ2djZDRvcWlhYWQzaTk4amh1MnBoMA==','Y2UzbWQyYWFkM2kxaDJuN24xODBjZTNtZDJhYWQzaTFoMm43bjE4Zw==','Y2R0ZG1yMmFkM2k0MXY3aG9nM2djZHRkbXIyYWQzaTQxdjdob2c0MA==',\n",
    "        'Y2U1NmViaWFkM2lmZHZ0aHQzcjBjZTU2ZWJpYWQzaWZkdnRodDNyZw==','Y2U0dTFiMmFkM2llMTg4dGY4bmdjZTR1MWIyYWQzaWUxODh0ZjhvMA==','Y2UzbWU2MmFkM2kxaDJuN24xc2djZTNtZTYyYWQzaTFoMm43bjF0MA==',\n",
    "        'Y2UzbWtiYWFkM2kxaDJuN240ZGdjZTNta2JhYWQzaTFoMm43bjRlMA==','Y2U1NjlxMmFkM2lmZHZ0aHQwZTBjZTU2OXEyYWQzaWZkdnRodDBlZw==','Y2UzbXYxYWFkM2kxaDJuN244dWdjZTNtdjFhYWQzaTFoMm43bjh2MA==',\n",
    "        'Y2U0dHE5YWFkM2llMTg4dGY0YzBjZTR0cTlhYWQzaWUxODh0ZjRjZw==','Y2U0dTQyaWFkM2llMTg4dGZhYWdjZTR1NDJpYWQzaWUxODh0ZmFiMA==','Y2U1NmQ2cWFkM2lmZHZ0aHQzM2djZTU2ZDZxYWQzaWZkdnRodDM0MA==']\n",
    "\n",
    "    finnhub_client_list = []\n",
    "\n",
    "    for key in finnhub_keys:\n",
    "        finnhub_client_list.append(finnhub.Client(api_key=base64.b64decode(key).decode()))\n",
    "\n",
    "    return finnhub_client_list\n",
    "\n",
    "finnhub_clients = generate_finnhub_clients()\n",
    "client_num = 0\n",
    "\n",
    "def get_finhub_client():\n",
    "    \"\"\" returns a finnhub client to perform requests to gather financial data \"\"\"\n",
    "\n",
    "    global client_num\n",
    "    if client_num >= len(finnhub_clients) - 1:\n",
    "        client_num = 0\n",
    "    else:\n",
    "        client_num+=1\n",
    "\n",
    "    return finnhub_clients[client_num]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google drive connection\n",
    "def get_sa_info():\n",
    "    sa_info = {\n",
    "        \"type\": \"service_account\",\n",
    "        \"project_id\": base64.b64decode('ZHNjaS01MTEtcHJvamVjdA==').decode(),\n",
    "        \"private_key_id\": base64.b64decode(\"OGU5OWQyY2I4YzAwNmMyM2ZjYTU4YTNiOTE4MzA3YmM4NmQ3MDg1Yg==\").decode(),\n",
    "        \"private_key\": base64.b64decode('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').decode(),\n",
    "        \"client_email\": base64.b64decode('ZHNjaS01MTFAZHNjaS01MTEtcHJvamVjdC5pYW0uZ3NlcnZpY2VhY2NvdW50LmNvbQ==').decode(),\n",
    "        \"client_id\": base64.b64decode('MTA4NTg5NzY5OTgzMDAzMTkwNTE2').decode(),\n",
    "        \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "        \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "        \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "        \"client_x509_cert_url\": base64.b64decode(\"aHR0cHM6Ly93d3cuZ29vZ2xlYXBpcy5jb20vcm9ib3QvdjEvbWV0YWRhdGEveDUwOS9kc2NpLTUxMSU0MGRzY2ktNTExLXByb2plY3QuaWFtLmdzZXJ2aWNlYWNjb3VudC5jb20=\").decode()\n",
    "    }\n",
    "\n",
    "    return sa_info\n",
    "\n",
    "sa_creds = Credentials.from_service_account_info(get_sa_info(), scopes=['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive'])\n",
    "gspread_client = gspread.authorize(sa_creds)\n",
    "drive_service = discovery.build('drive', 'v3', credentials=sa_creds)\n",
    "\n",
    "# google drive file IDs\n",
    "basic_financials_file_id = base64.b64decode(\"MTh4bWY2NktFV25aQ3NzbmRXVUsxTHpvVG0tUUt2OG1VUXVueWczS2JnUEE=\").decode()\n",
    "company_profile_file_id = base64.b64decode(\"MUhtT1BNWm85anpPRm55Vjd5ZlRCSXI4RjJyMGRxcU5XZEViaG9qc3Ezcmc=\").decode()\n",
    "trends_file_id = base64.b64decode(\"MTQwNmN6V0FFWmpKYjcxdEp6bDBoeVF3SGIycldNS0JfZk1BSGJyVm9oV0U=\").decode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(folder_id, ticker_symbol):\n",
    "    file_id = None\n",
    "    try:\n",
    "        file_create_metadata = {\n",
    "            'name': ticker_symbol,\n",
    "            'parents': [folder_id],\n",
    "            'mimeType': 'application/vnd.google-apps.spreadsheet',\n",
    "        }\n",
    "        file_id = drive_service.files().create(body=file_create_metadata).execute().get('id')\n",
    "\n",
    "    except HttpError as error:\n",
    "        print(f\"Error while creating file for {ticker_symbol} in folder {folder_id}, error: {error}\")\n",
    "        time.sleep(5)\n",
    "    \n",
    "    return file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_file(folder_id, ticker_symbol):\n",
    "    file_id = None\n",
    "    try:\n",
    "        query_params = {\"q\": \"'\" + folder_id + \"' in parents and mimeType != 'application/vnd.google-apps.folder'\"}\n",
    "        files = drive_service.files().list(**query_params).execute().get('files')\n",
    "\n",
    "        for file in files:\n",
    "            if file.get('mimeType') == 'application/vnd.google-apps.spreadsheet' and file.get('name') == ticker_symbol:\n",
    "                file_id = file.get('id')\n",
    "                break\n",
    "                \n",
    "    except HttpError as error:\n",
    "        print(f\"Error while searching folder {folder_id} for {ticker_symbol} file, error: {error}\")\n",
    "        time.sleep(5)\n",
    "    \n",
    "    return file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_or_create_file(folder_id, ticker_symbol):\n",
    "    file_id = find_file(folder_id, ticker_symbol)\n",
    "\n",
    "    # create new file when no file is found\n",
    "    if file_id == None:\n",
    "        file_id = create_file(folder_id, ticker_symbol)\n",
    "\n",
    "    return file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_existing_data_frame(file_id):\n",
    "\n",
    "    existing_google_sheet = gspread_client.open_by_key(file_id).sheet1\n",
    "\n",
    "    if existing_google_sheet.frozen_row_count == 0:\n",
    "        existing_google_sheet.freeze(rows=1)\n",
    "\n",
    "    if existing_google_sheet.row_count > 0:\n",
    "        existing_data_frame = gd.get_as_dataframe(existing_google_sheet)\n",
    "    else:\n",
    "        existing_data_frame = pd.DataFrame()\n",
    "\n",
    "\n",
    "    return existing_google_sheet, existing_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data_frame_to_empty_sheet(google_sheet, data_fame):\n",
    "    google_sheet.update([data_fame.columns.values.tolist()] + data_fame.values.tolist())\n",
    "    google_sheet.freeze(rows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_company_peers(ticker_symbol):\n",
    "    company_peers_response = get_finhub_client().company_peers(ticker_symbol)\n",
    "    company_peers = None\n",
    "\n",
    "    if company_peers_response and company_peers_response != None:\n",
    "        company_peer_list = list(company_peers_response)\n",
    "\n",
    "        if company_peer_list and len(company_peer_list) > 0:\n",
    "            company_peers = \" - \".join(company_peer_list)\n",
    "    \n",
    "    return company_peers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_profile_columns = ['symbol','name','country','finnhubIndustry','marketCapitalization','ipo','exchange','mic_code','security_type',\n",
    "        'shareOutstanding','currency','company_peers','logo','weburl','phone','figi_identifier','update_date']\n",
    "\n",
    "def populate_company_profile(ticker_symbol, stock_info):\n",
    "    \"\"\" populates the general information for the given company \"\"\"\n",
    "\n",
    "    company_profile = get_finhub_client().company_profile2(symbol=ticker_symbol)\n",
    "\n",
    "    profile_df = pd.json_normalize(company_profile)\n",
    "\n",
    "    if not profile_df.empty:\n",
    "\n",
    "        if 'ticker' in company_profile and len(company_profile['ticker']) > 0:\n",
    "            company_peers = retrieve_company_peers(ticker_symbol)\n",
    "\n",
    "            if company_peers and company_peers != None:\n",
    "                profile_df['company_peers'] = company_peers\n",
    "\n",
    "        if 'mic' in stock_info and len(stock_info['mic']) > 0:\n",
    "            profile_df['mic_code'] = stock_info['mic']\n",
    "\n",
    "        if 'figi' in stock_info and len(stock_info['figi']) > 0:\n",
    "            profile_df['figi_identifier'] = stock_info['figi']\n",
    "\n",
    "        if 'type' in stock_info and len(stock_info['type']) > 0:\n",
    "            profile_df['security_type'] = stock_info['type']\n",
    "\n",
    "        profile_df['update_date'] = today_date\n",
    "        profile_df.rename(columns={\"ticker\": \"symbol\"}, inplace=True)\n",
    "\n",
    "        try:\n",
    "            google_sheet, existing_df = retrieve_existing_data_frame(company_profile_file_id)\n",
    "\n",
    "            if not existing_df.empty:\n",
    "                updated_df = pd.concat([existing_df, profile_df], axis=0).sort_values(by=['symbol', 'update_date']).drop_duplicates(subset=['symbol'], keep='last')[company_profile_columns]\n",
    "                gd.set_with_dataframe(google_sheet, updated_df.dropna(how='all', axis=1))\n",
    "\n",
    "            else:\n",
    "                add_data_frame_to_empty_sheet(google_sheet, profile_df.sort_values(by=['symbol', 'update_date'])[company_profile_columns].dropna(how='all', axis=1))\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while generating company profile for {ticker_symbol}, error: {error}\")\n",
    "            time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_candlestick_data_frame(ticker_symbol):\n",
    "    \n",
    "    candle_response = get_finhub_client().stock_candles(symbol=ticker_symbol, resolution='D', _from=from_time_unix, to=to_time_unix)\n",
    "\n",
    "    candles = pd.json_normalize(candle_response)[['t', 'o', 'c', 'h', 'l', 'v']]\n",
    "    \n",
    "    candles_df = pd.DataFrame(columns=['date','unix_time', 'open', 'close', 'high', 'low', 'volume'])\n",
    "    \n",
    "    candles_df['unix_time'] = candles['t'][0]\n",
    "    candles_df['open'] = candles['o'][0]\n",
    "    candles_df['close'] = candles['c'][0]\n",
    "    candles_df['high'] = candles['h'][0]\n",
    "    candles_df['low'] = candles['l'][0]\n",
    "    candles_df['volume'] = candles['v'][0]\n",
    "\n",
    "    candles_df.sort_values(by=['unix_time'], ascending=False, inplace=True)\n",
    "    candles_df.drop_duplicates(subset=['unix_time'], keep='last', inplace=True)\n",
    "\n",
    "    candles_df['date'] = pd.to_datetime(candles_df['unix_time'],unit='s').astype(str)    \n",
    "\n",
    "    return candles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_candlestick_data(ticker_symbol):\n",
    "    \"\"\" populates the daily candlestick data for the given stock \"\"\"\n",
    "\n",
    "    candlestick_df = retrieve_candlestick_data_frame(ticker_symbol)\n",
    "\n",
    "    if not(candlestick_df.empty):\n",
    "        candlestick_df.insert(0,'symbol', ticker_symbol)\n",
    "\n",
    "        try:\n",
    "            google_sheet, existing_df = retrieve_existing_data_frame(\"1zvSOtP7x-g7RSsA0yZ3Uj0ZzUy6zHtdBQauZ9LLC4X4\")\n",
    "\n",
    "            if not existing_df.empty:\n",
    "                updated_df = pd.concat([existing_df, candlestick_df], axis=0).drop_duplicates(subset=['symbol', 'unix_time'], keep='last').sort_values(by=['symbol', 'unix_time'], ascending=[True, False])\n",
    "                gd.set_with_dataframe(google_sheet, updated_df.dropna(how='all', axis=1))\n",
    "                \n",
    "            else:\n",
    "                add_data_frame_to_empty_sheet(google_sheet, candlestick_df.sort_values(by=['symbol', 'unix_time'], ascending=[True, False]).dropna(how='all', axis=1))\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while updating candlestick data for {ticker_symbol}, error: {error}\")\n",
    "            time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_report_columns = ['symbol','year','concept','label','value','form','cik']\n",
    "financial_report_list = ['bs' , 'ic', 'cf']\n",
    "\n",
    "def populate_financials_reported(ticker_symbol):\n",
    "    \"\"\"\n",
    "    populates data into financials reported spreadsheet, the file generated will match the stock symbol using\n",
    "    the data provided from the financials as reported endpoint for the given company\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        financials = get_finhub_client().financials_reported(symbol=ticker_symbol)\n",
    "\n",
    "        google_sheet, existing_df = retrieve_existing_data_frame(find_or_create_file(\"1jRpA7By-7In1bSX1JkAbdlaQHaOyXz07\", ticker_symbol))\n",
    "        is_remote_df_empty = existing_df.empty\n",
    "\n",
    "        for report_key in financial_report_list:\n",
    "        \n",
    "            for fin_entry in financials['data']:\n",
    "\n",
    "                fin_report_df = pd.json_normalize(fin_entry['report'], record_path=[report_key])\n",
    "\n",
    "                if not fin_report_df.empty:\n",
    "                    fin_report_df.insert(0,'symbol', ticker_symbol)\n",
    "                    fin_report_df.insert(1,'year', fin_entry['year'])\n",
    "                    fin_report_df['form'] = fin_entry['form']\n",
    "                    fin_report_df['cik'] = fin_entry['cik']\n",
    "                    fin_report_df[fin_report_columns]\n",
    "\n",
    "                    if not existing_df.empty:\n",
    "                        existing_df = pd.concat([existing_df, fin_report_df], axis=0).drop_duplicates().sort_values(by=['symbol','year','label'], ascending=[True, False, True])[fin_report_columns]\n",
    "\n",
    "                    else:\n",
    "                        existing_df = fin_report_df.sort_values(by=['symbol','year','label'], ascending=[True, False, True])[fin_report_columns]\n",
    "\n",
    "\n",
    "        if not is_remote_df_empty:\n",
    "            gd.set_with_dataframe(google_sheet, existing_df)\n",
    "        else:\n",
    "            add_data_frame_to_empty_sheet(google_sheet, existing_df.sort_values(by=['symbol','year','label'], ascending=[True, False, True])[fin_report_columns])\n",
    "            \n",
    "    except Exception as error:\n",
    "        print(f\"Error while generating financials as reported for {ticker_symbol} error: {error}\")\n",
    "        time.sleep(10)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_social_columns = {\"mention\": \"mention_twitter\", \"positiveScore\": \"positiveScore_twitter\",\"negativeScore\": \"negativeScore_twitter\",\n",
    "    \"positiveMention\": \"positiveMention_twitter\",\"negativeMention\": \"negativeMention_twitter\",\"score\": \"score_twitter\"}\n",
    "\n",
    "def retrieve_social_sentiment_data_frame(ticker_symbol):\n",
    "\n",
    "    social_response = get_finhub_client().stock_social_sentiment(ticker_symbol)\n",
    "\n",
    "    twitter_social_df = pd.json_normalize(social_response, record_path='twitter')\n",
    "    reddit_social_df = pd.json_normalize(social_response, record_path='reddit')\n",
    "\n",
    "    try:\n",
    "        if 'atTime' in reddit_social_df:\n",
    "            social_df = twitter_social_df.merge(reddit_social_df, how='left', on=['atTime'], suffixes=('', '_reddit')).fillna(0).rename(columns=renamed_social_columns)\n",
    "\n",
    "        else:\n",
    "            social_df = twitter_social_df.fillna(0).rename(columns=renamed_social_columns)\n",
    "\n",
    "    except Exception as error:\n",
    "        print(f\"Error while gathering social sentiment for {ticker_symbol} error: {error}\")\n",
    "        social_df = twitter_social_df.fillna(0).rename(columns=renamed_social_columns)\n",
    "\n",
    "    return social_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_social_sentiment(ticker_symbol):\n",
    "    \"\"\" populates the social sentiment for stocks on Reddit and Twitter for the given stock \"\"\"\n",
    "\n",
    "    social_df = retrieve_social_sentiment_data_frame(ticker_symbol)\n",
    "    if not(social_df.empty):\n",
    "        \n",
    "        social_df.insert(0,'symbol', ticker_symbol)\n",
    "\n",
    "        try:\n",
    "            google_sheet, existing_df = retrieve_existing_data_frame(\"1-SXSPy03c9lAvSTBdSIl2up0eN1oQacnQXenInUopxo\")\n",
    "\n",
    "            if not existing_df.empty:\n",
    "                updated_df = pd.concat([existing_df, social_df], axis=0).drop_duplicates(subset=['symbol', 'atTime'], keep='last').sort_values(by=['symbol', 'atTime'], ascending=[True, False]).dropna(how='all', axis=1).fillna(0)\n",
    "                gd.set_with_dataframe(google_sheet, updated_df)\n",
    "\n",
    "            else:\n",
    "                add_data_frame_to_empty_sheet(google_sheet, social_df.sort_values(by=['symbol', 'atTime'], ascending=[True, False]).dropna(how='all', axis=1))\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while generating social sentiment data for {ticker_symbol}, error: {error}\")\n",
    "            time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insider_trans_cols =['symbol','share','change','transactionDate','transactionCode','transactionPrice','name','filingDate','id']\n",
    "\n",
    "def populate_insider_transactions(ticker_symbol):\n",
    "    \"\"\" populates insider transactions \"\"\"\n",
    "\n",
    "    insider_transactions_response = get_finhub_client().stock_insider_transactions(ticker_symbol)\n",
    "\n",
    "    insider_trans_df = pd.json_normalize(insider_transactions_response, record_path='data')\n",
    "\n",
    "    if not insider_trans_df.empty:\n",
    "        try:\n",
    "            google_sheet, existing_df = retrieve_existing_data_frame(\"1ivT0a63oHomkACLQIeq5tbFR2VET6dyRkQSsl6LWmoo\")\n",
    "\n",
    "            if not existing_df.empty:\n",
    "                updated_df = pd.concat([existing_df, insider_trans_df], axis=0).drop_duplicates().sort_values(by=['symbol','transactionDate'], ascending=[True, False]).dropna(how='all', axis=1)[insider_trans_cols]\n",
    "                gd.set_with_dataframe(google_sheet, updated_df)\n",
    "\n",
    "            else:\n",
    "                add_data_frame_to_empty_sheet(google_sheet, insider_trans_df.sort_values(by=['symbol','transactionDate'], ascending=[True, False]).dropna(how='all', axis=1)[insider_trans_cols])\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while generating insider transactions data for {ticker_symbol}, error: {error}\")\n",
    "            time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_cols =['symbol','period','actual','estimate','surprise','surprisePercent']\n",
    "\n",
    "def populate_surprise_earnings(ticker_symbol):\n",
    "    \"\"\" populates a company's surprise earnings \"\"\"\n",
    "\n",
    "    earnings_response = get_finhub_client().company_earnings(ticker_symbol)\n",
    "\n",
    "    earnings_df = pd.json_normalize(earnings_response)\n",
    "\n",
    "    if not earnings_df.empty:\n",
    "        try:\n",
    "            google_sheet, existing_df = retrieve_existing_data_frame(\"1h9ypbJSdq1E-y5B-7zAGtQI8pZovupYuEpjECLK95y0\")\n",
    "\n",
    "            if not existing_df.empty:\n",
    "                updated_df = pd.concat([existing_df, earnings_df], axis=0).drop_duplicates(subset=['symbol', 'period']).sort_values(by=['symbol','period'], ascending=[True, False]).dropna(how='all', axis=1)[earnings_cols]\n",
    "                gd.set_with_dataframe(google_sheet, updated_df)\n",
    "\n",
    "            else:\n",
    "                add_data_frame_to_empty_sheet(google_sheet, earnings_df.sort_values(by=['symbol','period'], ascending=[True, False]).dropna(how='all', axis=1)[earnings_cols])\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while generating company surprise earnings data for {ticker_symbol}, error: {error}\")\n",
    "            time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_basic_financials(ticker_symbol):\n",
    "    \"\"\" populates the basic financials of a company \"\"\"\n",
    "\n",
    "    basic_financials_response = get_finhub_client().company_basic_financials(ticker_symbol, 'all')\n",
    "\n",
    "    if 'metric' in basic_financials_response and 'symbol' in basic_financials_response:\n",
    "\n",
    "        basic_fin_df = pd.DataFrame([basic_financials_response['metric']])\n",
    "        basic_fin_df.insert(0,'symbol', basic_financials_response['symbol'])\n",
    "        basic_fin_df['update_date'] = today_date\n",
    "\n",
    "        try:\n",
    "            google_sheet, existing_df = retrieve_existing_data_frame(basic_financials_file_id)\n",
    "\n",
    "            if not existing_df.empty:\n",
    "                updated_df = pd.concat([existing_df, basic_fin_df], axis=0).sort_values(by=['symbol', 'update_date'], ascending=[True, False]).drop_duplicates(subset=['symbol'], keep='last').dropna(how='all', axis=1)\n",
    "                gd.set_with_dataframe(google_sheet, updated_df)\n",
    "\n",
    "            else:\n",
    "                add_data_frame_to_empty_sheet(google_sheet, basic_fin_df.dropna(how='all', axis=1))\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while updating basic financial data for {ticker_symbol}, error: {error}\")\n",
    "            time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_insider_sentiment(ticker_symbol):\n",
    "    \"\"\" populates the insider sentiment data \"\"\"\n",
    "\n",
    "    insider_response = get_finhub_client().stock_insider_sentiment(ticker_symbol, \"2010-01-01\", current_year+\"-12-31\")\n",
    "\n",
    "    insider_df = pd.json_normalize(insider_response, record_path='data')\n",
    "\n",
    "    if not insider_df.empty:\n",
    "        try:\n",
    "            google_sheet, existing_df = retrieve_existing_data_frame(\"197PQPaYIJeCCYGHJreqA1duHcPfL5Ql5IhVj2JdSyuA\")\n",
    "\n",
    "            if not existing_df.empty:\n",
    "                updated_df = pd.concat([existing_df, insider_df], axis=0).drop_duplicates().sort_values(by=['symbol','year','month'], ascending=[True,False,False]).dropna(how='all', axis=1)\n",
    "                gd.set_with_dataframe(google_sheet, updated_df)\n",
    "\n",
    "            else:\n",
    "                add_data_frame_to_empty_sheet(google_sheet, insider_df.sort_values(by=['symbol','year','month'], ascending=[True,False,False]).dropna(how='all', axis=1))\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while generating insider sentiment data for {ticker_symbol}, error: {error}\")\n",
    "            time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_columns = ['symbol', 'period', 'strongBuy', 'buy', 'hold', 'sell', 'strongSell']\n",
    "\n",
    "def populate_recommended_trends(ticker_symbol):\n",
    "    \"\"\" populates the latest analyst recommendation trends for a company \"\"\"\n",
    "\n",
    "    trends = get_finhub_client().recommendation_trends(symbol=ticker_symbol)\n",
    "    trends_df = pd.json_normalize(trends)\n",
    "\n",
    "    if not trends_df.empty:\n",
    "\n",
    "        trends_df[trends_columns]\n",
    "\n",
    "        try:\n",
    "            google_sheet, existing_df = retrieve_existing_data_frame(trends_file_id)\n",
    "            \n",
    "            if not existing_df.empty:\n",
    "                updated_df = pd.concat([existing_df, trends_df], axis=0).sort_values(by=['symbol', 'period'], ascending=[True, False]).drop_duplicates(subset=['symbol', 'period'])[trends_columns]\n",
    "                gd.set_with_dataframe(google_sheet, updated_df.dropna(how='all', axis=1))\n",
    "\n",
    "            else:\n",
    "                add_data_frame_to_empty_sheet(google_sheet, trends_df.sort_values(by=['symbol', 'period'], ascending=[True, False]).dropna(how='all', axis=1))\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while updating recommendation trends data for {ticker_symbol}, error: {error}\")\n",
    "            time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_senate_lobbying(ticker_symbol):\n",
    "    \"\"\" populates the reported lobbying activities in the Senate and the House \"\"\"\n",
    "\n",
    "    lobby_response = get_finhub_client().stock_lobbying(ticker_symbol, \"2000-01-01\", today_date)\n",
    "\n",
    "    lobby_df = pd.json_normalize(lobby_response, record_path='data')\n",
    "\n",
    "    if not lobby_df.empty:\n",
    "\n",
    "        symbol_col = lobby_df.pop('symbol')\n",
    "        year_col = lobby_df.pop('year')\n",
    "        lobby_df.insert(0, 'symbol', symbol_col)\n",
    "        lobby_df.insert(0, 'year', year_col)\n",
    "\n",
    "        try:\n",
    "            google_sheet, existing_df = retrieve_existing_data_frame(\"1rY_9WQ5F80BEzpoVRbIM739sXqrbEj84XJ7EllbvCF0\")\n",
    "\n",
    "            if not existing_df.empty:\n",
    "                updated_df = pd.concat([existing_df, lobby_df], axis=0).sort_values(by=['symbol', 'year'], ascending=[True, False]).drop_duplicates().dropna(how='all', axis=1)\n",
    "                gd.set_with_dataframe(google_sheet, updated_df)\n",
    "                \n",
    "            else:\n",
    "                add_data_frame_to_empty_sheet(google_sheet, lobby_df.dropna(how='all', axis=1))\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while generating senate lobbying data for {ticker_symbol}, error: {error}\")\n",
    "            time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_generate_data_for_ticker(stock_info, symbol_input, stock_tickers):\n",
    "    \n",
    "    is_stock_info_valid = 'symbol' in stock_info and len(stock_info['symbol']) > 0\n",
    "\n",
    "    if is_stock_info_valid:\n",
    "\n",
    "        if len(stock_tickers) > 0 and stock_info['symbol'] in stock_tickers:\n",
    "            return True\n",
    "\n",
    "        has_symbol_input = symbol_input and len(symbol_input) > 0\n",
    "\n",
    "        if not(has_symbol_input):\n",
    "            return True\n",
    "        \n",
    "        if has_symbol_input and symbol_input == stock_info['symbol']:\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_user_for_symbol_input():\n",
    "\n",
    "    user_symbol_input = None\n",
    "    \n",
    "    print(\"-- Enter Stock Symbol In Prompt -- \\nIf no input is provided data will be generated from a pre-defined set of stocks\")\n",
    "\n",
    "    try:\n",
    "        user_symbol_input = input(\"Enter Stock Symbol or Nothing for Nasdaq Stocks:\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"An error occured while waiting for input. Data will be generated multiple stocks\")\n",
    "    \n",
    "\n",
    "    if user_symbol_input and len(user_symbol_input) > 0:\n",
    "        print(f\"Attempting to Generate Data Stock Symbol: {user_symbol_input}\")\n",
    "    else:\n",
    "        print(\"No Stock Symbol Provided.\")\n",
    "\n",
    "    return user_symbol_input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_generate_data_for_ticker2(stock_info, stock_tickers):\n",
    "    return 'symbol' in stock_info and len(stock_info['symbol']) > 0 and stock_info['symbol'] in stock_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieves all stock ticker symbols matching the params\n",
    "all_stock_info = get_finhub_client().stock_symbols(exchange=\"US\", currency=\"USD\", security_type=\"Common Stock\", mic=\"XNAS\")\n",
    "# mic=\"XNAS\" - Nasdaq, mic=\"XNYS\" - New York Stock Exchange\n",
    "\n",
    "stock_tickers = {}\n",
    "\n",
    "symbol_input = prompt_user_for_symbol_input()\n",
    "\n",
    "print(\"Please Wait... Generating Financial Data. Interrupt the Program to Exit\")\n",
    "processed_symbols = []\n",
    "\n",
    "for stock_info in all_stock_info:\n",
    "    try:\n",
    "        if should_generate_data_for_ticker(stock_info, stock_tickers, stock_tickers):\n",
    "            ticker_symbol = stock_info['symbol']\n",
    "\n",
    "            populate_company_profile(ticker_symbol, stock_info)\n",
    "            \n",
    "            populate_candlestick_data(ticker_symbol)\n",
    "            \n",
    "            populate_financials_reported(ticker_symbol)\n",
    "            \n",
    "            populate_social_sentiment(ticker_symbol)\n",
    "\n",
    "            populate_insider_transactions(ticker_symbol)\n",
    "\n",
    "            populate_surprise_earnings(ticker_symbol)\n",
    "\n",
    "            populate_basic_financials(ticker_symbol)\n",
    "\n",
    "            populate_insider_sentiment(ticker_symbol)\n",
    "            \n",
    "            populate_recommended_trends(ticker_symbol)\n",
    "\n",
    "            populate_senate_lobbying(ticker_symbol)\n",
    "\n",
    "            processed_symbols.append(ticker_symbol)\n",
    "\n",
    "            if len(processed_symbols) % 5 == 0:\n",
    "                print(f\"{len(processed_symbols)} Symbols Processed. Current Run Time: {(time.time() - start_time) / 60} minutes\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while generating data for {ticker_symbol}, error: {e}\")\n",
    "        time.sleep(10)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Process interrupted\")\n",
    "        break\n",
    "\n",
    "print(f\"-- Exiting Program -- Total Execution Time: {(time.time() - start_time) / 60} minutes\")\n",
    "print(f\"{len(processed_symbols)} Stock Symbols Processed. Symbol List: {processed_symbols}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
